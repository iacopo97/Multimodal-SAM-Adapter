<!-- # MM SAM-Adapter -->

<!-- 
The official implementation of the paper "[Vision Transformer Adapter for Dense Predictions](https://arxiv.org/abs/2205.08534)".

[Paper](https://arxiv.org/abs/2205.08534) | [Blog in Chinese](https://zhuanlan.zhihu.com/p/608272954) | [Slides](https://drive.google.com/file/d/1LotIZIEnZzKhsANjBTZs3qcezk9fbVCV/view?usp=share_link) | [Poster](https://iclr.cc/media/PosterPDFs/ICLR%202023/12048.png?t=1680764158.7068026) | [Video in English](https://iclr.cc/virtual/2023/poster/12048) | [Video in Chinese](https://www.bilibili.com/video/BV1ry4y1976b/?spm_id_from=333.337.search-card.all.click)

[Segmentation Colab Notebook](https://colab.research.google.com/drive/1yEd5lQMjShloicImtShkwttb74KPGY5U?usp=sharing) | [Detection Colab Notebook](https://colab.research.google.com/drive/1Im7l0dSvEgsP-AJtUOxgbU9a1C3DdSwe?usp=sharing) (thanks [@IamShubhamGupto](https://github.com/IamShubhamGupto), [@dudifrid](https://github.com/dudifrid))

## News



## Abstract



## Method

<img width="810" alt="image" src="https://user-images.githubusercontent.com/23737120/217998186-8a37eacb-18f8-445a-8d92-0863e35712ab.png">

<img width="810" alt="image" src="https://user-images.githubusercontent.com/23737120/194904786-ea9c40a3-f6ac-4fe1-90ad-976e7b9e8f03.png">

## Catalog
- [ ] Support flash attention
- [ ] Support faster deformable attention
- [x] Segmentation checkpoints
- [x] Segmentation code
- [x] Detection checkpoints
- [x] Detection code
- [x] Initialization

## Awesome Competition Solutions with ViT-Adapter

**[1st Place Solution for the 5th LSVOS Challenge: Video Instance Segmentation](https://arxiv.org/abs/2306.04091)**
</br>
Tao Zhang, Xingye Tian, Yikang Zhou, Yuehua Wu, Shunping Ji, Cilin Yan, Xuebo Wang, Xin Tao, Yuanhui Zhang, Pengfei Wan
</br>
[[`Code`](https://github.com/zhang-tao-whu/DVIS)]
[![Star](https://img.shields.io/github/stars/zhang-tao-whu/DVIS.svg?style=social&label=Star)]([https://github.com/zhang-tao-whu/DVIS])
</br>
August 28, 2023

**2nd place solution in Scene Understanding for Autonomous Drone Delivery (SUADD'23) competition**
</br>
Mykola Lavreniuk, Nivedita Rufus, Unnikrishnan R Nair
</br>
[[`Code`](https://github.com/Lavreniuk/2nd-place-solution-in-Scene-Understanding-for-Autonomous-Drone-Delivery)]
[![Star](https://img.shields.io/github/stars/Lavreniuk/2nd-place-solution-in-Scene-Understanding-for-Autonomous-Drone-Delivery.svg?style=social&label=Star)]([https://github.com/Lavreniuk/2nd-place-solution-in-Scene-Understanding-for-Autonomous-Drone-Delivery])
</br>
July 18, 2023

**Champion solution in Track 3 (3D Occupancy Prediction) of the CVPR 2023 Autonomous Driving Challenge**
</br>
[FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation](https://arxiv.org/abs/2307.01492)
</br>
Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, Jose M. Alvarez
</br>
[[`Code`](https://github.com/NVlabs/FB-BEV)]
[![Star](https://img.shields.io/github/stars/NVlabs/FB-BEV.svg?style=social&label=Star)]([https://github.com/NVlabs/FB-BEV])
</br>
June 26, 2023 


**[3rd Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation](https://arxiv.org/abs/2306.06753)**
</br>
Jinming Su, Wangwang Yang, Junfeng Luo, Xiaolin Wei
</br>
June 6, 2023 

**Champion solution in the Video Scene Parsing in the Wild Challenge at CVPR 2023**
</br>
[Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach](https://arxiv.org/abs/2306.03508)
</br>
Min Yan, Qianxiong Ning, Qian Wang
</br>
June 3, 2023 

**2nd place in the Video Scene Parsing in the Wild Challenge at CVPR 2023**
</br>
[Recyclable Semi-supervised Method Based on Multi-model Ensemble for Video Scene Parsing](https://arxiv.org/abs/2306.02894)
</br>
Biao Wu, Shaoli Liu, Diankai Zhang, Chengjian Zheng, Si Gao, Xiaofeng Zhang, Ning Wang
</br>
June 2, 2023 


**[Champion Solution for the WSDM2023 Toloka VQA Challenge](https://arxiv.org/abs/2301.09045)**
</br>
Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu
</br>
[[`Code`](https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023)]
</br>
January 9, 2023

**[1st Place Solutions for the UVO Challenge 2022](https://arxiv.org/abs/2210.09629)**
</br>
Jiajun Zhang, Boyu Chen, Zhilong Ji, Jinfeng Bai, Zonghai Hu
</br>
October 9, 2022


## Citation

If this work is helpful for your research, please consider citing the following BibTeX entry.

```
@article{chen2022vitadapter,
  title={Vision Transformer Adapter for Dense Predictions},
  author={Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  journal={arXiv preprint arXiv:2205.08534},
  year={2022}
}
``` -->


<h1 align="center"> Multimodal SAM-Adapter for Semantic Segmentation </h1> 


<br>
:rotating_light: This repository contains download links to dataset, code snippets, and trained deep models of our work  "Multimodal SAM-Adapter for Semantic Segmentation", 
 
by [Iacopo Curti*](https://www.unibo.it/sitoweb/iacopo.curti2), [Pierluigi Zama Ramirez*](https://pierlui92.github.io/), Alioscia Petrelli*
, and [Luigi Di Stefano*](https://www.unibo.it/sitoweb/luigi.distefano).  \* _Equal Contribution_

University of Bologna


<div class="alert alert-info">


<h2 align="center"> 

<!-- [Project Page](https://cvlab-unibo.github.io/Depth4ToM/) | [Paper](https://arxiv.org/abs/2307.15052) -->
</h2>


## :bookmark_tabs: Table of Contents

1. [Introduction](#clapper-introduction)
2. [Dataset](#file_cabinet-dataset)
   - [Download](#arrow_down-get-your-hands-on-the-data)
3. [Pretrained Models](#inbox_tray-pretrained-models)
4. [Code](#memo-code)
5. [Checkpoints](#floppy_disk-checkpoints)
5. [Quantitative Results](#bar_chart-quantitative-results)
6. [Qualitative Results](#art-qualitative-results)
7. [Contacts](#envelope-contacts)


<!-- </div> -->
## :clapper: Introduction
</div>
<!-- Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks. We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation. We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model. These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces. Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal. -->
Semantic segmentation, an essential task for applications such as autonomous driving, medical imaging,
and robotics, has advanced enormously with deep learning but still struggles in challenging conditions
such as poor lighting and adverse weather. Multimodal approaches incorporating auxiliary sensor data (e.g.,
LiDAR, infrared) have emerged to address these limitations. This work introduces MM SAM-adapter, a novel
framework that leverages the Segment Anything Model (SAM) for multimodal semantic segmentation. Our
approach consists of an adapter network that injects multimodal fused features into SAMâ€™s rich RGB features.
This strategy allows the model to prioritize RGB information while selectively incorporating auxiliary
data when beneficial. We conduct extensive experiments on multimodal benchmarks such as DeLiVER,
FMB, and MUSES, where MM SAM-adapter achieves state-of-the-art performance. Additionally, we
manually divide DeLIVER and FMB into RGB-easy and RGB-hard splits, to evaluate better the impact of
employing synergistically auxiliary modalities. Our results demonstrate that MM SAM-adapter outperforms
competitors in easy and challenging scenarios.
<h4 align="center">

</h4>

<img src="./images/BISAMAD_b.png" alt="Alt text" style="width: 800px;" title="architecture">

<!-- :fountain_pen: If you find this code useful in your research, please cite:

```bibtex
@inproceedings{costanzino2023iccv,
    title = {Learning Depth Estimation for Transparent and Mirror Surfaces},
    author = {Costanzino, Alex and Zama Ramirez, Pierluigi and Poggi, Matteo and Tosi, Fabio and Mattoccia, Stefano and Di Stefano, Luigi},
    booktitle = {The IEEE International Conference on Computer Vision},
    note = {ICCV},
    year = {2023},
}
``` -->

## :file_cabinet: Dataset

In our experiments, we employed three datasets featuring different modalities: DeLiVER, MUSES and FMB. Those datasets have been downloaded from the corresponding repositories. See /datasets folder for further details.

<!-- ### :arrow_down: Get Your Hands on the Data
Trans10K and MSD with Virtual Depths. [[Download]](https://1drv.ms/u/s!AgV49D1Z6rmGgZAz2I7tMepfdVrZYQ?e=jbuaJB)

We also employed the Booster Dataset in our experiment. [[Download]](https://cvlab-unibo.github.io/booster-web/) -->

## :inbox_tray: Pretrained Models

In order to download the SAM backbone that we have used during the paper it is needed to run the script:
```
python /segmentation/tools/SAM_checkpoint_convert.py
```
<!-- Here, you can download the weights of **MiDAS** and **DPT** architectures employed in the results of Table 2 and Table 3 of our paper. If you just need the best model, use `"Table 2/Ft. Virtual Depth/dpt_large_final.pt` -->

To use these weights, please follow these steps:

1. Create a folder named `pretrained` in the segmentation directory.
3. Copy the downloaded weights into the `pretrained` folder.

## :memo: Code

<!-- <div class="alert alert-info">

**Warning**:
- Please be aware that we will not be releasing the training code for deep stereo models. We provide only our algorithm to obtain proxy depth labels by merging monocular and stereo predictions.
- The code utilizes `wandb` during training to log results. Please be sure to have a wandb account. Otherwise, if you prefer to not use `wandb`, comment the wandb logging code lines in `finetune.py`. -->

<!-- </div> -->


### :hammer_and_wrench: Setup Instructions

<!-- **Dependencies**: Ensure that you have installed all the necessary dependencies. The list of dependencies can be found in the `./requirements.txt` file. -->
Install [MMSegmentation v0.20.2](https://github.com/open-mmlab/mmsegmentation/tree/v0.20.2) following the instructions:

```
cd segmentation
# recommended environment: torch1.9 + cuda11.1
conda create -n MMSAMAD python==3.9
conda activate MMSAMAD
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
pip install mmcv-full==1.4.2 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html
pip install timm==0.4.12
pip install numpy==1.24.3
pip install yapf==0.40.1
pip install mmsegmentation==0.20.2
pip install tqdm
<<<<<<< HEAD
pip install "opencv-python<4.9.0"
=======
pip install opencv-python<4.9.0
pip install scipy
pip install einops==0.8.0
>>>>>>> b6b57c6130235595d4e14f81ade7a4e44f7fbe85
cd ops
bash make.sh # compile deformable attention
```
You should also create a folder called data within the segmentation folder where a symbolic path to each dataset is created:
```
ln -s /path/to/DELIVER /data
ln -s /path/to/muses /data
ln -s /path/to/FMB /data
```

<!-- ### :rocket: Inference Monocular Networks

The `run.py` script test monocular networks. It can be used to predict the monocular depth maps from pretrained networks, or to apply our in-painting strategy of Base networks to obtain Virtual Depths.

You can specify the following options:
   - `--input_path`: Path to the root directory of the dataset. E.g., _Booster/balanced/train_ if you want to test the model on the training set of Booster.
   - `--dataset_txt`: The list of the dataset samples. Each line contains the relative path to `input_path` of each image. You can find some examples in the folder _datasets/_. E.g., to run on the training set of booster use *datasets\booster\train_stereo.txt*
   - `--mask_path`: Optional path with the folder containing masks. Each mask shoud have the same relative path of the corresponding image. When this path is specified, masks are applied to colorize ToM objects.
   - `--cls2mask`: IDs referring to ToM objects in masks.
   - `--it`: Number of inferences for each image. Used when in-painting with several random colors.
   - `--output_path`: Output directory,
   - `--output_list`: Save the prediction paths in a txt file.
   - `--save_full_res`: Save the prediction at the input resolution. If not specified save the predictions at the model output resolution.
   - `--model_weights`: Path to the trained weights of the model. If not specified load the Base network weights from default paths.
   - `--model_type`: Model type. Either `dpt_large` or `midas_v21`.

You can reproduce the results of Table 2 and Table 3 of the paper by running `scripts/table2.sh` and `scripts/table3.sh`.

If you haven't downloaded the pretrained models yet, you can find the download links in the **Pretrained Models** section above. -->

### :rocket: Train our network
To train MM SAM-Adapter + Segformer head on DeLiVER on a single node with 2 gpus run:
```
cd segmentation
bash dist_train.sh configs/DELIVER/Segformer_MMSAM_adapter_large_DELIVER_1024x1024_ss_RGBLIDAR.py 2
```

### :rocket: Evaluation of our network

To evaluate MM SAM-Adapter + Segformer head on DeLiVER test set on a single node with 2 gpus run:
```
cd segmentation
bash dist_test.sh configs/DELIVER/Segformer_MMSAM_adapter_large_DELIVER_1024x1024_ss_RGBLIDAR.py /path/to/checkpoint_file 1 --eval mIoU --show-dir /visualization_directory --resize-dim 1024 1024 #resize-dim is (800,600) in case of FMB
```
To evaluate MM SAM-Adapter + Segformer head on DeLiVER test set in easy/hard split on a single node with 2 gpus run. In order to do this kind of evaluation it is needed to change the test dict within the config file specifying the type of dataset needed, i.e. DELIVER_easy:
```
cd segmentation
bash dist_test.sh configs/Segformer_MMSAM_adapter_large_DELIVER_1024x1024_ss_RGBLIDAR_easy.py /path/to/checkpoint_file 1 --eval mIoU --show-dir /visualization_directory --resize-dim 1024 1024 #resize-dim is (800,600) in case of FMB
```

### :rocket: Infer of MUSES test set

```
cd segmentation
bash dist_infer.sh configs/Segformer_MMSAM_adapter_large_MUSES_1024x1024_ss_RGBLIDAR.py /path/to/checkpoint_file 1 --eval mIoU --show-dir /inference_directory --resize-dim 1080 1920 #resize-dim is (800,600) in case of FMB
```

## :floppy_disk: Checkpoints

Download pretrained checkpoints for MM SAM-Adapter from the links below and place them in a specific folder (i.e. /work_dirs).

**DELIVER**
| Method           | Backbone    | Crop Size | Input Modalities      | mIoU      | Download                                                                                                              |
|------------------|-------------|-----------|-----------------------|-----------|-----------------------------------------------------------------------------------------------------------------------|
| MM SAM-Adapter   | ConvNext-S  | 1024      | RGB + LiDAR          |     57.14      | [ckpt](https://drive.google.com/file/d/1sez-j2Tgb9lPxZbtyIfNRrb_MF6Kla8e/view?usp=sharing) |
| MM SAM-Adapter   | ConvNext-S  | 1024      | RGB + Depth          |      57.35     | [ckpt](https://drive.google.com/file/d/1lY1vYrGPeESEvb7FX6wSCPPcLJJ3k1Eo/view?usp=sharing) |
| MM SAM-Adapter   | ConvNext-S  | 1024      | RGB + Event          |      55.70     | [ckpt](https://drive.google.com/file/d/1KnL5lzdIKM99nIT42dB8jbXcpUfdJkvp/view?usp=sharing) |

**FMB**
| Method           | Backbone    | Crop Size | Input Modalities      | mIoU      | Download                                                                                                              |
|------------------|-------------|-----------|-----------------------|-----------|-----------------------------------------------------------------------------------------------------------------------|
| MM SAM-Adapter   | ConvNext-S  | 800      | RGB + Thermal          |     66.10      | [ckpt](https://drive.google.com/file/d/1w_A5rdLgpIMhtQ_QSKJb2cEN2akePgY0/view?usp=sharing) |


**MUSES**
| Method           | Backbone    | Crop Size | Input Modalities      | mIoU      | Download                                                                                                              |
|------------------|-------------|-----------|-----------------------|-----------|-----------------------------------------------------------------------------------------------------------------------|
| MM SAM-Adapter   | ConvNext-S  | 1024      | RGB + LiDAR          |     81.07      | [ckpt](https://drive.google.com/file/d/1QZ31283E10aXymF6VaptmuL7oEvL2irR/view?usp=sharing) |
| MM SAM-Adapter   | ConvNext-S  | 1024      | RGB + Event          |     79.92      | [ckpt](https://drive.google.com/file/d/1S7BatJmGHWKZo6TGcCn-OS7wHGRiY82B/view?usp=sharing) |

<!-- To finetune networks refer to the example in `scripts/finetune.sh` -->

<!-- ### :rocket: Monocular Virtual Depth Generation

To generate virtual depth from depth networks using our in-paiting strategy refer to the example in `scripts/generate_virtual_depth.sh`

### :rocket: Stereo Proxy Depth Generation

To generate proxy depth maps with our merging strategy to finetune stereo networks you can use `create_proxy_stereo.py`. 

As explained above, we will not release the code for finetuning stereo networks. However, our implementation was based on the official codes of [RAFT-Stereo](https://github.com/princeton-vl/RAFT-Stereo) and [CREStereo](https://github.com/megvii-research/CREStereo). -->
## :bar_chart: Quantitative Results

In this section, we present the main quantitative results presented in our paper.
The following two tables shows  DeLiVER test set results in the RGB-Depth (RGB-D), RGB-LiDAR (RGB-L), and RGB-Event (RGB-E) setups (RIGHT) and  FMB test set results in the RGB-Thermal (RGB-T) setup across
different scenarios (LEFT).
<p float="left">
<img src="./images/FMB_DELIVER_TABLES.png" alt="GIF" width="800" >
</p>
The following table shows MUSES test results in the RGB-LiDAR (RGB-L) and RGB-Event (RGB-E) setups for different weather conditions.
<p float="left">
<img src="./images/MUSES_table.png" alt="GIF" width="800" >
</p>



## :art: Qualitative Results

In this section, we present illustrative examples that demonstrate the effectiveness of our proposal.

<p float="left">
<img src="./images/qualitatives_DELIVER_tot_a.png" alt="GIF" width="800" >
</p>
<p float="left">
<img src="./images/qualitatives_REAL_b.png" alt="GIF" width="800" >
</p>

## :envelope: Contacts

For questions, please send an email to iacopo.curti2@unibo.it, pierluigi.zama@unibo.it


## License

This repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE.md) file.



## :pray: Acknowledgements

We would like to extend our sincere appreciation to the authors of the following projects for making their code available, which we have utilized in our work:

- We would like to thank the authors of [DeLiVER](https://github.com/jamycheung/DELIVER), [ViT-Adapter](https://github.com/czczup/ViT-Adapter), [MUSES](https://github.com/timbroed/MUSES) for providing their code, which has been instrumental in our experiments.

We deeply appreciate the authors of the competing research papers for their helpful responses, and provision of model weights, which greatly aided accurate comparisons.